{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef61543c-d595-40a5-8e20-4cb31d2b2c5d",
   "metadata": {},
   "source": [
    "# Test Probabilisitc Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424cf1c4-6463-4f3f-bfe4-d7009e1932d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e8a7bf4-ae7d-44b0-b33f-59fc13549f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import visualization as viz\n",
    "from models import (\n",
    "    MeanStdUNet,\n",
    "    MedianScaleUNet,\n",
    "    BinClassifierUNet,\n",
    "    QuantileRegressorUNet,\n",
    "    MonteCarloDropoutUNet,\n",
    "    DeterministicUNet,\n",
    "    IQUNetPipeline,\n",
    "    UNetConfig,\n",
    "    MixtureDensityUNet,\n",
    ")\n",
    "from metrics.deterministic_metrics import relative_rmse, relative_mae\n",
    "from metrics.crps import crps_laplace, crps_gaussian\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b713e77-018e-4fe6-95ac-91e18cd78677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c7dc6-bfeb-4bf2-ad7d-b95c70f2cf28",
   "metadata": {},
   "source": [
    "## Mixture Density Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5172c1be-67d1-4fd6-870f-5c38cd66346c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for 60 min time horizon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GOES16Dataset:Number of sequences filtered: 614\n",
      "INFO:GOES16Dataset:Number of sequences filtered by black images: 1\n",
      "INFO:GOES16Dataset:Number of sequences filtered: 192\n",
      "INFO:GOES16Dataset:Number of sequences filtered by black images: 1\n",
      "INFO:MixtureDensityUNet:Train loader size: 23247\n",
      "INFO:MixtureDensityUNet:Val loader size: 4666\n",
      "INFO:MixtureDensityUNet:Samples height: 1024, Samples width: 1024\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"../checkpoints/goes16/mdn/MixDensityUNet_IN3_F16_NC3_SC6_BS_6_TH60_E3_BVM0_02_D2024-10-12_19:15.pt\"\n",
    "n_components = 3\n",
    "\n",
    "unet_config = UNetConfig(\n",
    "    in_frames=3,\n",
    "    spatial_context=0,\n",
    "    filters=16,\n",
    "    output_activation=\"sigmoid\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "mdn_unet = MixtureDensityUNet(\n",
    "    config=unet_config,\n",
    "    n_components=n_components,\n",
    ")\n",
    "\n",
    "mdn_unet.load_checkpoint(checkpoint_path, device, eval_mode=True)\n",
    "print(f\"Trained for {mdn_unet.time_horizon} min time horizon\")\n",
    "mdn_unet.create_dataloaders(\n",
    "    dataset=\"goes16\",\n",
    "    path=\"../datasets/goes16/salto/\",\n",
    "    batch_size=1,\n",
    "    time_horizon=mdn_unet.time_horizon,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58400b65-b0bd-46b1-8f6d-c0778432453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "pred_params shape: torch.Size([1, 9, 1024, 1024])\n",
      "points shape: torch.Size([1, 200, 1024, 1024])\n",
      "pis: tensor([0.3410, 0.3398, 0.3191], device='cuda:0')\n",
      "mus: tensor([0.4767, 0.5306, 0.4926], device='cuda:0')\n",
      "sigmas: tensor([0.9520, 0.9911, 0.9505], device='cuda:0')\n",
      "tensor(0.5421, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "numeric_crps_list = []\n",
    "with torch.no_grad():\n",
    "    for val_batch_idx, (in_frames, out_frames) in enumerate(\n",
    "        mdn_unet.val_loader\n",
    "    ):\n",
    "\n",
    "        # in_frames = in_frames.to(device=device, dtype=probabilistic_unet.torch_dtype)\n",
    "        in_frames = in_frames.to(device=device)\n",
    "        # out_frames = out_frames.to(device=device, dtype=probabilistic_unet.torch_dtype)\n",
    "        out_frames = out_frames.to(device=device)\n",
    "\n",
    "        # with torch.autocast(device_type=\"cuda\", dtype=probabilistic_unet.torch_dtype):\n",
    "        #     frames_pred = probabilistic_unet.model(in_frames)\n",
    "        frames_pred = mdn_unet.model(in_frames.float())\n",
    "        print(frames_pred.shape)\n",
    "        frames_pred = mdn_unet.mdn_forward(frames_pred)\n",
    "        print(frames_pred.shape)\n",
    "        numeric_crps = mdn_unet.get_numerical_CRPS(\n",
    "            y=out_frames,\n",
    "            pred=frames_pred,\n",
    "            lower=0,\n",
    "            upper=1,\n",
    "            count=200,\n",
    "        )\n",
    "        numeric_crps_list.append(numeric_crps)\n",
    "        print(numeric_crps)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ebd98-938f-4a8d-a649-2acd29c4eb54",
   "metadata": {},
   "source": [
    "## Implicit Quantile Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33b2c7b-a9a2-4f31-92d0-4730594c75e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for None min time horizon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GOES16Dataset:Number of sequences filtered: 614\n",
      "INFO:GOES16Dataset:Number of sequences filtered by black images: 1\n",
      "INFO:GOES16Dataset:Number of sequences filtered: 192\n",
      "INFO:GOES16Dataset:Number of sequences filtered by black images: 1\n",
      "INFO:IQUNetPipeline:Train loader size: 23247\n",
      "INFO:IQUNetPipeline:Val loader size: 4666\n",
      "INFO:IQUNetPipeline:Samples height: 1024, Samples width: 1024\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"../checkpoints/goes16/iqn_F32_TH60/iqn/IQUNet_IN3_F32_NT9_CED64_PD0_BS_4_TH60_E1_BVM0_27_D2024-11-05_01:22.pt\"\n",
    "\n",
    "unet_config = UNetConfig(\n",
    "    in_frames=3,\n",
    "    spatial_context=0,\n",
    "    filters=32,\n",
    "    output_activation=\"\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "iqn_unet = IQUNetPipeline(\n",
    "    config=unet_config,\n",
    ")\n",
    "\n",
    "iqn_unet.load_checkpoint(checkpoint_path, device, eval_mode=True)\n",
    "print(f\"Trained for {iqn_unet.time_horizon} min time horizon\")\n",
    "iqn_unet.create_dataloaders(\n",
    "    dataset=\"goes16\",\n",
    "    path=\"../datasets/goes16/salto/\",\n",
    "    batch_size=1,\n",
    "    time_horizon=60,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bab5d739-59ce-4750-82f9-02e82edf8233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1218, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5291, 0.5288, 0.5292, 0.5294, 0.5280, 0.5294, 0.5286, 0.5292, 0.5303],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0901, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5678, 0.5664, 0.5658, 0.5671, 0.5713, 0.5713, 0.5701, 0.5652, 0.5716],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0897, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5238, 0.5239, 0.5234, 0.5239, 0.5228, 0.5239, 0.5234, 0.5235, 0.5247],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.7485, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6845, 0.6932, 0.6924, 0.6868, 0.6891, 0.6932, 0.6917, 0.6793, 0.6859],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2017, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5310, 0.5317, 0.5312, 0.5327, 0.5311, 0.5324, 0.5303, 0.5318, 0.5325],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1164, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5256, 0.5256, 0.5252, 0.5259, 0.5248, 0.5256, 0.5251, 0.5258, 0.5268],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1190, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5263, 0.5260, 0.5260, 0.5265, 0.5252, 0.5261, 0.5255, 0.5263, 0.5275],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1228, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5313, 0.5314, 0.5320, 0.5326, 0.5312, 0.5326, 0.5314, 0.5322, 0.5333],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1300, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5397, 0.5411, 0.5409, 0.5408, 0.5403, 0.5425, 0.5400, 0.5400, 0.5420],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1674, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5474, 0.5468, 0.5485, 0.5472, 0.5490, 0.5494, 0.5482, 0.5471, 0.5489],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2267, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5511, 0.5497, 0.5519, 0.5507, 0.5532, 0.5533, 0.5516, 0.5502, 0.5527],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.4805, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6013, 0.6025, 0.6051, 0.6042, 0.6076, 0.6066, 0.6045, 0.6023, 0.6050],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6260, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6404, 0.6422, 0.6453, 0.6439, 0.6464, 0.6455, 0.6430, 0.6419, 0.6447],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.8242, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6786, 0.6816, 0.6858, 0.6830, 0.6849, 0.6853, 0.6819, 0.6810, 0.6831],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1425, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5324, 0.5326, 0.5327, 0.5335, 0.5315, 0.5330, 0.5318, 0.5324, 0.5341],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6753, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6481, 0.6511, 0.6531, 0.6518, 0.6538, 0.6536, 0.6514, 0.6490, 0.6535],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1439, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5402, 0.5402, 0.5408, 0.5408, 0.5405, 0.5420, 0.5398, 0.5401, 0.5420],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6372, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6351, 0.6377, 0.6396, 0.6379, 0.6413, 0.6411, 0.6382, 0.6373, 0.6397],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1412, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5857, 0.5869, 0.5892, 0.5885, 0.5904, 0.5897, 0.5881, 0.5865, 0.5896],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.9326, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6832, 0.6951, 0.6927, 0.6752, 0.6860, 0.6972, 0.6955, 0.6517, 0.6768],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2137, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5559, 0.5588, 0.5604, 0.5584, 0.5605, 0.5599, 0.5588, 0.5580, 0.5591],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6465, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6644, 0.6673, 0.6706, 0.6681, 0.6702, 0.6695, 0.6670, 0.6660, 0.6692],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1068, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5491, 0.5482, 0.5496, 0.5485, 0.5505, 0.5512, 0.5497, 0.5488, 0.5507],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1392, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5347, 0.5356, 0.5357, 0.5360, 0.5344, 0.5363, 0.5347, 0.5351, 0.5368],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.5322, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6522, 0.6571, 0.6589, 0.6559, 0.6585, 0.6580, 0.6553, 0.6560, 0.6574],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1057, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5344, 0.5373, 0.5363, 0.5389, 0.5386, 0.5402, 0.5402, 0.5325, 0.5401],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.9644, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6897, 0.7025, 0.6978, 0.6884, 0.6921, 0.6999, 0.7002, 0.6750, 0.6877],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1086, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5277, 0.5276, 0.5274, 0.5279, 0.5268, 0.5278, 0.5270, 0.5278, 0.5290],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1043, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5265, 0.5266, 0.5262, 0.5268, 0.5257, 0.5266, 0.5261, 0.5267, 0.5279],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0935, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5247, 0.5245, 0.5242, 0.5247, 0.5237, 0.5247, 0.5241, 0.5242, 0.5256],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1052, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5247, 0.5247, 0.5242, 0.5249, 0.5239, 0.5248, 0.5242, 0.5247, 0.5257],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.5342, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6304, 0.6330, 0.6351, 0.6333, 0.6380, 0.6375, 0.6342, 0.6331, 0.6352],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1285, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5360, 0.5373, 0.5371, 0.5373, 0.5362, 0.5382, 0.5360, 0.5365, 0.5382],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1020, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5770, 0.5797, 0.5825, 0.5806, 0.5841, 0.5828, 0.5817, 0.5786, 0.5819],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1132, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5297, 0.5293, 0.5297, 0.5303, 0.5291, 0.5302, 0.5290, 0.5298, 0.5313],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2177, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5693, 0.5699, 0.5738, 0.5721, 0.5756, 0.5737, 0.5723, 0.5692, 0.5730],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1227, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5306, 0.5307, 0.5312, 0.5313, 0.5301, 0.5316, 0.5306, 0.5313, 0.5322],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0959, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5230, 0.5230, 0.5224, 0.5231, 0.5221, 0.5231, 0.5225, 0.5226, 0.5236],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6738, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6539, 0.6561, 0.6596, 0.6569, 0.6593, 0.6585, 0.6558, 0.6552, 0.6577],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2168, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5383, 0.5386, 0.5390, 0.5388, 0.5383, 0.5403, 0.5383, 0.5385, 0.5402],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1216, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5314, 0.5317, 0.5318, 0.5325, 0.5304, 0.5325, 0.5308, 0.5320, 0.5331],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1166, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5322, 0.5346, 0.5331, 0.5346, 0.5333, 0.5335, 0.5318, 0.5326, 0.5342],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1176, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5315, 0.5311, 0.5309, 0.5324, 0.5306, 0.5321, 0.5303, 0.5315, 0.5325],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.3264, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5931, 0.5978, 0.5983, 0.5975, 0.6001, 0.5988, 0.5976, 0.5966, 0.5987],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1240, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5300, 0.5306, 0.5306, 0.5316, 0.5298, 0.5310, 0.5295, 0.5307, 0.5317],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1230, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5291, 0.5288, 0.5291, 0.5293, 0.5281, 0.5294, 0.5284, 0.5292, 0.5303],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1421, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6083, 0.6076, 0.6111, 0.6099, 0.6134, 0.6120, 0.6098, 0.6083, 0.6112],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2539, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5339, 0.5341, 0.5334, 0.5339, 0.5338, 0.5347, 0.5331, 0.5324, 0.5348],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2064, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5483, 0.5523, 0.5535, 0.5510, 0.5514, 0.5514, 0.5516, 0.5500, 0.5513],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1351, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5315, 0.5321, 0.5322, 0.5326, 0.5308, 0.5328, 0.5311, 0.5320, 0.5331],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.4570, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6857, 0.7001, 0.6953, 0.6768, 0.6880, 0.6999, 0.6998, 0.6517, 0.6784],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.4746, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6175, 0.6166, 0.6198, 0.6198, 0.6234, 0.6223, 0.6192, 0.6184, 0.6199],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.5698, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6543, 0.6571, 0.6596, 0.6575, 0.6598, 0.6595, 0.6570, 0.6550, 0.6590],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.5996, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6560, 0.6613, 0.6628, 0.6598, 0.6627, 0.6625, 0.6597, 0.6597, 0.6616],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1064, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5272, 0.5272, 0.5272, 0.5281, 0.5269, 0.5276, 0.5269, 0.5283, 0.5289],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0662, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5253, 0.5251, 0.5252, 0.5263, 0.5253, 0.5253, 0.5252, 0.5265, 0.5271],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1158, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5296, 0.5296, 0.5297, 0.5306, 0.5290, 0.5303, 0.5289, 0.5300, 0.5307],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1270, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5342, 0.5353, 0.5351, 0.5360, 0.5348, 0.5363, 0.5341, 0.5349, 0.5362],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1185, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5272, 0.5271, 0.5268, 0.5272, 0.5261, 0.5271, 0.5266, 0.5270, 0.5285],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0936, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5241, 0.5241, 0.5236, 0.5241, 0.5230, 0.5241, 0.5235, 0.5236, 0.5249],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0985, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5476, 0.5465, 0.5478, 0.5471, 0.5489, 0.5492, 0.5475, 0.5466, 0.5490],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2852, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5440, 0.5451, 0.5466, 0.5452, 0.5468, 0.5469, 0.5460, 0.5439, 0.5461],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6514, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6872, 0.6939, 0.6944, 0.6915, 0.6923, 0.6939, 0.6926, 0.6884, 0.6905],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1148, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5286, 0.5284, 0.5286, 0.5291, 0.5281, 0.5290, 0.5279, 0.5293, 0.5301],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1081, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5250, 0.5250, 0.5245, 0.5249, 0.5238, 0.5249, 0.5244, 0.5245, 0.5260],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1494, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5424, 0.5428, 0.5432, 0.5429, 0.5429, 0.5449, 0.5425, 0.5423, 0.5445],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1092, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5250, 0.5250, 0.5244, 0.5249, 0.5238, 0.5249, 0.5244, 0.5246, 0.5259],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2091, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5272, 0.5272, 0.5274, 0.5282, 0.5269, 0.5276, 0.5269, 0.5285, 0.5290],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1038, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5259, 0.5264, 0.5263, 0.5272, 0.5263, 0.5266, 0.5262, 0.5278, 0.5281],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1111, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5240, 0.5240, 0.5235, 0.5241, 0.5231, 0.5241, 0.5235, 0.5236, 0.5248],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1588, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5310, 0.5313, 0.5313, 0.5323, 0.5301, 0.5319, 0.5303, 0.5314, 0.5326],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.3826, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5405, 0.5425, 0.5419, 0.5412, 0.5425, 0.5438, 0.5431, 0.5386, 0.5427],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1261, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5241, 0.5245, 0.5242, 0.5250, 0.5237, 0.5243, 0.5242, 0.5253, 0.5259],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1554, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5351, 0.5391, 0.5370, 0.5384, 0.5379, 0.5394, 0.5379, 0.5336, 0.5389],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1092, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5253, 0.5252, 0.5249, 0.5254, 0.5243, 0.5253, 0.5247, 0.5250, 0.5264],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6914, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5611, 0.5621, 0.5650, 0.5642, 0.5655, 0.5648, 0.5630, 0.5605, 0.5642],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1464, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5324, 0.5332, 0.5333, 0.5338, 0.5320, 0.5339, 0.5322, 0.5332, 0.5344],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.7915, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6571, 0.6605, 0.6632, 0.6604, 0.6642, 0.6635, 0.6599, 0.6604, 0.6617],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1736, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5258, 0.5258, 0.5253, 0.5258, 0.5248, 0.5258, 0.5253, 0.5256, 0.5269],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1216, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5574, 0.5604, 0.5620, 0.5593, 0.5624, 0.5617, 0.5611, 0.5581, 0.5613],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1178, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5262, 0.5264, 0.5263, 0.5270, 0.5260, 0.5265, 0.5260, 0.5273, 0.5279],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1011, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5271, 0.5269, 0.5268, 0.5272, 0.5261, 0.5271, 0.5265, 0.5269, 0.5284],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1875, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5381, 0.5387, 0.5390, 0.5391, 0.5385, 0.5400, 0.5381, 0.5383, 0.5402],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0925, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5243, 0.5243, 0.5239, 0.5243, 0.5232, 0.5244, 0.5238, 0.5237, 0.5252],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.4553, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6697, 0.6735, 0.6738, 0.6723, 0.6740, 0.6758, 0.6734, 0.6684, 0.6768],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1335, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5337, 0.5348, 0.5350, 0.5354, 0.5337, 0.5356, 0.5338, 0.5344, 0.5361],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1722, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5280, 0.5278, 0.5278, 0.5283, 0.5273, 0.5282, 0.5273, 0.5283, 0.5293],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1098, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5294, 0.5295, 0.5296, 0.5299, 0.5289, 0.5300, 0.5289, 0.5302, 0.5311],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1221, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5255, 0.5262, 0.5260, 0.5268, 0.5261, 0.5264, 0.5260, 0.5273, 0.5277],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1267, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5306, 0.5309, 0.5310, 0.5320, 0.5297, 0.5316, 0.5299, 0.5312, 0.5322],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1110, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5261, 0.5261, 0.5256, 0.5261, 0.5250, 0.5261, 0.5255, 0.5259, 0.5273],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1215, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5325, 0.5333, 0.5334, 0.5338, 0.5321, 0.5341, 0.5323, 0.5331, 0.5344],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1108, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5270, 0.5271, 0.5267, 0.5272, 0.5262, 0.5272, 0.5265, 0.5272, 0.5284],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0944, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5282, 0.5301, 0.5293, 0.5298, 0.5289, 0.5295, 0.5282, 0.5280, 0.5298],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1131, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5275, 0.5274, 0.5273, 0.5280, 0.5270, 0.5278, 0.5269, 0.5281, 0.5289],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1219, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5288, 0.5286, 0.5288, 0.5291, 0.5278, 0.5292, 0.5284, 0.5290, 0.5301],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1123, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5278, 0.5274, 0.5274, 0.5276, 0.5266, 0.5278, 0.5269, 0.5273, 0.5289],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1229, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5282, 0.5280, 0.5283, 0.5286, 0.5276, 0.5286, 0.5276, 0.5286, 0.5295],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0975, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5262, 0.5263, 0.5258, 0.5263, 0.5252, 0.5262, 0.5257, 0.5261, 0.5275],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1260, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5279, 0.5278, 0.5277, 0.5282, 0.5273, 0.5282, 0.5273, 0.5282, 0.5292],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1638, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5558, 0.5595, 0.5605, 0.5583, 0.5603, 0.5605, 0.5597, 0.5575, 0.5598],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1200, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5273, 0.5272, 0.5271, 0.5277, 0.5267, 0.5276, 0.5267, 0.5277, 0.5286],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1429, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5328, 0.5333, 0.5331, 0.5335, 0.5331, 0.5335, 0.5327, 0.5326, 0.5339],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.7617, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5442, 0.5454, 0.5481, 0.5454, 0.5477, 0.5471, 0.5468, 0.5439, 0.5465],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0941, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5243, 0.5249, 0.5246, 0.5256, 0.5244, 0.5249, 0.5246, 0.5261, 0.5263],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6880, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6697, 0.6728, 0.6760, 0.6731, 0.6750, 0.6746, 0.6720, 0.6710, 0.6744],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.5684, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6512, 0.6540, 0.6565, 0.6549, 0.6569, 0.6566, 0.6544, 0.6524, 0.6563],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2661, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5390, 0.5396, 0.5399, 0.5395, 0.5393, 0.5413, 0.5392, 0.5390, 0.5409],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2474, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5244, 0.5241, 0.5240, 0.5246, 0.5231, 0.5240, 0.5239, 0.5243, 0.5256],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1569, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5471, 0.5484, 0.5496, 0.5480, 0.5494, 0.5501, 0.5489, 0.5478, 0.5494],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1984, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5615, 0.5629, 0.5645, 0.5632, 0.5655, 0.5649, 0.5637, 0.5616, 0.5640],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1201, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5348, 0.5351, 0.5357, 0.5357, 0.5347, 0.5363, 0.5348, 0.5349, 0.5366],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1116, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5263, 0.5263, 0.5259, 0.5263, 0.5252, 0.5263, 0.5258, 0.5260, 0.5275],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1361, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5402, 0.5402, 0.5407, 0.5406, 0.5406, 0.5420, 0.5403, 0.5403, 0.5421],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0891, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5243, 0.5243, 0.5238, 0.5244, 0.5233, 0.5243, 0.5238, 0.5240, 0.5252],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2991, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5830, 0.5871, 0.5881, 0.5869, 0.5903, 0.5889, 0.5873, 0.5859, 0.5884],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(1., device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6624, 0.6653, 0.6683, 0.6661, 0.6683, 0.6677, 0.6651, 0.6638, 0.6673],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1726, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5330, 0.5337, 0.5329, 0.5325, 0.5328, 0.5343, 0.5329, 0.5301, 0.5332],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1158, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5282, 0.5279, 0.5278, 0.5282, 0.5272, 0.5283, 0.5274, 0.5280, 0.5294],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1011, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5260, 0.5260, 0.5257, 0.5262, 0.5252, 0.5262, 0.5256, 0.5258, 0.5272],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1241, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5271, 0.5273, 0.5270, 0.5283, 0.5277, 0.5279, 0.5265, 0.5279, 0.5284],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1031, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5262, 0.5261, 0.5258, 0.5261, 0.5251, 0.5262, 0.5256, 0.5258, 0.5274],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0927, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5237, 0.5235, 0.5232, 0.5237, 0.5227, 0.5238, 0.5231, 0.5229, 0.5244],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1305, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5376, 0.5387, 0.5390, 0.5392, 0.5378, 0.5398, 0.5379, 0.5378, 0.5401],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1225, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5285, 0.5281, 0.5283, 0.5287, 0.5278, 0.5288, 0.5277, 0.5285, 0.5297],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1362, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5301, 0.5302, 0.5306, 0.5310, 0.5296, 0.5310, 0.5301, 0.5309, 0.5318],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1296, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5387, 0.5371, 0.5377, 0.5386, 0.5382, 0.5392, 0.5380, 0.5372, 0.5394],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6846, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6504, 0.6532, 0.6555, 0.6541, 0.6560, 0.6557, 0.6536, 0.6516, 0.6555],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1174, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5284, 0.5281, 0.5282, 0.5285, 0.5275, 0.5286, 0.5276, 0.5282, 0.5297],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1248, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5309, 0.5308, 0.5313, 0.5315, 0.5306, 0.5319, 0.5307, 0.5312, 0.5322],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2218, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5537, 0.5569, 0.5589, 0.5565, 0.5588, 0.5580, 0.5573, 0.5555, 0.5572],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.5918, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6325, 0.6346, 0.6369, 0.6365, 0.6387, 0.6379, 0.6354, 0.6338, 0.6371],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1214, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5283, 0.5280, 0.5281, 0.5284, 0.5274, 0.5285, 0.5275, 0.5283, 0.5295],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1349, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5500, 0.5498, 0.5502, 0.5496, 0.5504, 0.5517, 0.5509, 0.5482, 0.5506],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1313, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5298, 0.5299, 0.5301, 0.5307, 0.5287, 0.5304, 0.5293, 0.5303, 0.5313],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.4617, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5961, 0.5967, 0.5997, 0.5985, 0.6017, 0.6000, 0.5982, 0.5974, 0.5994],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6245, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5387, 0.5386, 0.5392, 0.5392, 0.5391, 0.5405, 0.5386, 0.5388, 0.5405],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1304, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5293, 0.5290, 0.5293, 0.5296, 0.5285, 0.5297, 0.5286, 0.5295, 0.5306],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.4082, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6049, 0.6066, 0.6082, 0.6083, 0.6113, 0.6105, 0.6076, 0.6079, 0.6086],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6392, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5845, 0.5830, 0.5854, 0.5842, 0.5888, 0.5872, 0.5855, 0.5838, 0.5869],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1611, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5373, 0.5374, 0.5380, 0.5379, 0.5378, 0.5390, 0.5371, 0.5373, 0.5389],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1090, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5249, 0.5249, 0.5245, 0.5252, 0.5241, 0.5250, 0.5244, 0.5249, 0.5260],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1223, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5313, 0.5313, 0.5319, 0.5325, 0.5311, 0.5326, 0.5314, 0.5321, 0.5331],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1078, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5342, 0.5355, 0.5354, 0.5362, 0.5349, 0.5363, 0.5344, 0.5350, 0.5365],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.4504, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6014, 0.6044, 0.6067, 0.6053, 0.6089, 0.6069, 0.6054, 0.6037, 0.6060],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0932, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5249, 0.5249, 0.5246, 0.5249, 0.5239, 0.5250, 0.5244, 0.5244, 0.5259],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.9233, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6833, 0.6897, 0.6888, 0.6765, 0.6853, 0.6927, 0.6906, 0.6600, 0.6807],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1375, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5374, 0.5384, 0.5387, 0.5384, 0.5375, 0.5396, 0.5376, 0.5378, 0.5397],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.8564, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6878, 0.7031, 0.6970, 0.6869, 0.6900, 0.6995, 0.7002, 0.6719, 0.6854],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1014, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5242, 0.5242, 0.5236, 0.5242, 0.5231, 0.5241, 0.5236, 0.5238, 0.5250],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0817, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5216, 0.5217, 0.5210, 0.5215, 0.5204, 0.5216, 0.5211, 0.5208, 0.5220],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1676, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5590, 0.5659, 0.5658, 0.5623, 0.5610, 0.5625, 0.5634, 0.5614, 0.5646],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1355, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5382, 0.5392, 0.5394, 0.5391, 0.5384, 0.5404, 0.5384, 0.5386, 0.5404],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.3428, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5280, 0.5277, 0.5270, 0.5288, 0.5264, 0.5280, 0.5262, 0.5280, 0.5289],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6914, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6521, 0.6539, 0.6573, 0.6557, 0.6585, 0.6574, 0.6550, 0.6539, 0.6566],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0941, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5245, 0.5245, 0.5240, 0.5246, 0.5235, 0.5245, 0.5239, 0.5242, 0.5254],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1768, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5479, 0.5477, 0.5467, 0.5470, 0.5494, 0.5516, 0.5524, 0.5425, 0.5516],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.4270, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5491, 0.5508, 0.5526, 0.5505, 0.5518, 0.5519, 0.5513, 0.5503, 0.5516],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1101, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5531, 0.5524, 0.5542, 0.5525, 0.5548, 0.5558, 0.5544, 0.5530, 0.5549],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1272, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5298, 0.5297, 0.5300, 0.5304, 0.5288, 0.5304, 0.5294, 0.5302, 0.5312],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1304, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5314, 0.5324, 0.5327, 0.5334, 0.5326, 0.5329, 0.5315, 0.5326, 0.5333],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1063, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5249, 0.5249, 0.5244, 0.5248, 0.5236, 0.5248, 0.5243, 0.5243, 0.5258],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6099, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6423, 0.6426, 0.6463, 0.6447, 0.6481, 0.6470, 0.6437, 0.6431, 0.6453],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1327, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5314, 0.5319, 0.5319, 0.5325, 0.5309, 0.5327, 0.5311, 0.5322, 0.5331],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1058, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5249, 0.5248, 0.5243, 0.5248, 0.5237, 0.5248, 0.5243, 0.5244, 0.5258],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.7754, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6636, 0.6659, 0.6688, 0.6668, 0.6688, 0.6685, 0.6659, 0.6644, 0.6680],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1166, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5298, 0.5299, 0.5297, 0.5311, 0.5299, 0.5306, 0.5290, 0.5303, 0.5309],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.5039, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5901, 0.5902, 0.5911, 0.5922, 0.5960, 0.5950, 0.5933, 0.5899, 0.5948],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.8545, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6685, 0.6720, 0.6796, 0.6735, 0.6781, 0.6777, 0.6728, 0.6704, 0.6725],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1047, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5255, 0.5255, 0.5251, 0.5256, 0.5245, 0.5255, 0.5250, 0.5253, 0.5266],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1383, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5389, 0.5401, 0.5400, 0.5398, 0.5395, 0.5415, 0.5392, 0.5390, 0.5409],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1140, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5276, 0.5274, 0.5272, 0.5276, 0.5266, 0.5276, 0.5269, 0.5274, 0.5288],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1387, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5330, 0.5346, 0.5343, 0.5349, 0.5333, 0.5351, 0.5331, 0.5337, 0.5352],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1648, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5547, 0.5581, 0.5595, 0.5576, 0.5600, 0.5591, 0.5584, 0.5576, 0.5586],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1852, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5359, 0.5364, 0.5373, 0.5377, 0.5380, 0.5381, 0.5364, 0.5367, 0.5382],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.8203, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6859, 0.6956, 0.6936, 0.6887, 0.6901, 0.6941, 0.6933, 0.6818, 0.6872],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0802, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5219, 0.5220, 0.5213, 0.5220, 0.5209, 0.5219, 0.5214, 0.5214, 0.5224],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.4536, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5621, 0.5643, 0.5663, 0.5657, 0.5678, 0.5672, 0.5664, 0.5638, 0.5664],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0982, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5241, 0.5241, 0.5237, 0.5240, 0.5229, 0.5241, 0.5236, 0.5235, 0.5250],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.7266, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6558, 0.6575, 0.6613, 0.6589, 0.6618, 0.6607, 0.6576, 0.6573, 0.6595],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0959, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5504, 0.5515, 0.5521, 0.5504, 0.5512, 0.5523, 0.5517, 0.5509, 0.5525],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0889, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5219, 0.5219, 0.5213, 0.5219, 0.5208, 0.5219, 0.5214, 0.5213, 0.5223],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1132, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5277, 0.5275, 0.5274, 0.5279, 0.5268, 0.5278, 0.5270, 0.5278, 0.5290],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1233, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5365, 0.5364, 0.5371, 0.5371, 0.5366, 0.5379, 0.5362, 0.5366, 0.5382],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1294, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5309, 0.5315, 0.5313, 0.5322, 0.5303, 0.5322, 0.5303, 0.5316, 0.5325],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1148, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5285, 0.5282, 0.5283, 0.5288, 0.5279, 0.5288, 0.5278, 0.5285, 0.5298],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.8047, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6537, 0.6549, 0.6589, 0.6562, 0.6591, 0.6581, 0.6550, 0.6548, 0.6569],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0937, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5252, 0.5252, 0.5248, 0.5254, 0.5244, 0.5254, 0.5248, 0.5252, 0.5263],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0974, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5233, 0.5233, 0.5227, 0.5234, 0.5223, 0.5233, 0.5228, 0.5230, 0.5240],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1234, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5312, 0.5314, 0.5316, 0.5321, 0.5304, 0.5323, 0.5307, 0.5317, 0.5327],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1163, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5378, 0.5385, 0.5386, 0.5387, 0.5383, 0.5399, 0.5379, 0.5380, 0.5397],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.2052, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6030, 0.6080, 0.6073, 0.6071, 0.6098, 0.6097, 0.6077, 0.6074, 0.6092],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.7339, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5682, 0.5780, 0.5722, 0.5729, 0.5769, 0.5767, 0.5786, 0.5664, 0.5794],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1089, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5662, 0.5663, 0.5683, 0.5670, 0.5686, 0.5686, 0.5672, 0.5662, 0.5673],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1393, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5290, 0.5288, 0.5292, 0.5295, 0.5281, 0.5294, 0.5286, 0.5294, 0.5304],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1028, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5250, 0.5250, 0.5244, 0.5250, 0.5239, 0.5249, 0.5244, 0.5247, 0.5259],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1105, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5256, 0.5257, 0.5252, 0.5259, 0.5249, 0.5257, 0.5252, 0.5258, 0.5268],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1133, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5296, 0.5302, 0.5304, 0.5310, 0.5301, 0.5310, 0.5297, 0.5309, 0.5314],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.6919, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6578, 0.6602, 0.6618, 0.6610, 0.6635, 0.6638, 0.6605, 0.6583, 0.6632],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1152, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5285, 0.5284, 0.5282, 0.5287, 0.5270, 0.5286, 0.5274, 0.5280, 0.5295],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1879, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5838, 0.5871, 0.5896, 0.5881, 0.5912, 0.5893, 0.5883, 0.5863, 0.5887],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1049, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5271, 0.5269, 0.5267, 0.5273, 0.5262, 0.5271, 0.5264, 0.5271, 0.5284],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1077, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5466, 0.5487, 0.5485, 0.5474, 0.5492, 0.5501, 0.5494, 0.5450, 0.5488],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1069, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5665, 0.5668, 0.5670, 0.5671, 0.5691, 0.5694, 0.5680, 0.5647, 0.5692],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1571, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5318, 0.5322, 0.5323, 0.5330, 0.5316, 0.5332, 0.5317, 0.5327, 0.5336],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.7314, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6522, 0.6554, 0.6573, 0.6560, 0.6578, 0.6577, 0.6555, 0.6531, 0.6576],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1231, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5220, 0.5219, 0.5219, 0.5227, 0.5215, 0.5222, 0.5219, 0.5221, 0.5232],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.4529, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.6725, 0.6770, 0.6769, 0.6783, 0.6788, 0.6808, 0.6776, 0.6725, 0.6807],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1283, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5384, 0.5388, 0.5392, 0.5395, 0.5399, 0.5401, 0.5386, 0.5389, 0.5404],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1722, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5628, 0.5669, 0.5683, 0.5666, 0.5693, 0.5683, 0.5676, 0.5656, 0.5678],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1267, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5305, 0.5305, 0.5310, 0.5315, 0.5302, 0.5315, 0.5303, 0.5313, 0.5322],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.1105, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5268, 0.5267, 0.5264, 0.5267, 0.5256, 0.5267, 0.5262, 0.5264, 0.5280],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 1, 1024, 1024])\n",
      "torch.Size([1, 9, 1024, 1024])\n",
      "tensor(0.0917, device='cuda:0', dtype=torch.float16)\n",
      "tensor([0.5233, 0.5233, 0.5229, 0.5232, 0.5221, 0.5233, 0.5228, 0.5225, 0.5240],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m quantile_loss_per_batch \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# stores values for this validation run\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val_batch_idx, (in_frames, out_frames) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m      5\u001b[0m         iqn_unet\u001b[38;5;241m.\u001b[39mval_loader\n\u001b[1;32m      6\u001b[0m     ):\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# in_frames = in_frames.to(device=device, dtype=iqn_unet.torch_dtype)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# out_frames = out_frames.to(device=device, dtype=iqn_unet.torch_dtype)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# with torch.autocast(device_type=\"cuda\", dtype=iqn_unet.torch_dtype):\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m#     print(iqn_unet.val_quantiles)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m#     frames_pred = iqn_unet.model(in_frames, iqn_unet.val_quantiles)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m#     frames_pred = iqn_unet.remove_spatial_context(frames_pred)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m#     print(out_frames.shape)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m#     print(frames_pred.shape)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m#     print(out_frames[0, 0, 512, 512])\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m#     print(frames_pred[0, :, 512, 512])\u001b[39;00m\n\u001b[1;32m     21\u001b[0m             \n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m#     quantile_loss = iqn_unet.calculate_loss(\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m#         frames_pred, out_frames, iqn_unet.val_quantiles\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m#     quantile_loss_per_batch.append(quantile_loss.detach().item())\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         in_frames \u001b[38;5;241m=\u001b[39m in_frames\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     28\u001b[0m         out_frames \u001b[38;5;241m=\u001b[39m out_frames\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/prob-cloud/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/prob-cloud/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/prob-cloud/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/prob-cloud/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/cloudiness_probabilistic_forecasting/src/data_handlers/data.py:113\u001b[0m, in \u001b[0;36mGOES16Dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    111\u001b[0m     in_frames \u001b[38;5;241m=\u001b[39m in_frames[np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_in_images:  \u001b[38;5;66;03m# next images in in_frames\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     aux \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     aux \u001b[38;5;241m=\u001b[39m aux[np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m    120\u001b[0m     in_frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((in_frames, aux), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/prob-cloud/lib/python3.9/site-packages/numpy/lib/npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/miniconda3/envs/prob-cloud/lib/python3.9/site-packages/numpy/lib/format.py:809\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m    822\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "quantile_loss_per_batch = []  # stores values for this validation run\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_batch_idx, (in_frames, out_frames) in enumerate(\n",
    "        iqn_unet.val_loader\n",
    "    ):\n",
    "\n",
    "        in_frames = in_frames.to(device=device, dtype=iqn_unet.torch_dtype)\n",
    "        out_frames = out_frames.to(device=device, dtype=iqn_unet.torch_dtype)\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=iqn_unet.torch_dtype):\n",
    "            print(iqn_unet.val_quantiles)\n",
    "            frames_pred = iqn_unet.model(in_frames, iqn_unet.val_quantiles)\n",
    "            frames_pred = iqn_unet.remove_spatial_context(frames_pred)\n",
    "\n",
    "            # print(out_frames.shape)\n",
    "            # print(frames_pred.shape)\n",
    "\n",
    "            # print(out_frames[0, 0, 512, 512])\n",
    "            # print(frames_pred[0, :, 512, 512])\n",
    "            \n",
    "            quantile_loss = iqn_unet.calculate_loss(\n",
    "                frames_pred, out_frames, iqn_unet.val_quantiles\n",
    "            )\n",
    "            quantile_loss_per_batch.append(quantile_loss.detach().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cacb47a-a6ec-4723-81fa-08f3c784dc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantile los in validation: 1.484743088879812\n"
     ]
    }
   ],
   "source": [
    "quantile_loss_in_epoch = sum(quantile_loss_per_batch) / len(quantile_loss_per_batch)\n",
    "print(f\"quantile los in validation: {quantile_loss_in_epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e26410-adca-4951-bc30-7beb8b37f29c",
   "metadata": {},
   "source": [
    "## Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ae1e2d-3c32-4246-97a7-caad72227ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for 60 min time horizon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GOES16Dataset:Number of sequences filtered: 614\n",
      "INFO:GOES16Dataset:Number of sequences filtered by black images: 1\n",
      "INFO:GOES16Dataset:Number of sequences filtered: 192\n",
      "INFO:GOES16Dataset:Number of sequences filtered by black images: 1\n",
      "INFO:MedianScaleUNet:Train loader size: 23247\n",
      "INFO:MedianScaleUNet:Val loader size: 4666\n",
      "INFO:MedianScaleUNet:Samples height: 1024, Samples width: 1024\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"../checkpoints/goes16/median_60/median/MedianScaleUNet_IN3_F32_SC0_BS_4_TH60_E16_BVM0_55_D2024-11-01_19:14.pt\"\n",
    "\n",
    "unet_config = UNetConfig(\n",
    "    in_frames=3,\n",
    "    spatial_context=0,\n",
    "    filters=32,\n",
    "    output_activation=\"sigmoid\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "med_unet = MedianScaleUNet(\n",
    "    config=unet_config,\n",
    ")\n",
    "\n",
    "med_unet.load_checkpoint(checkpoint_path, device, eval_mode=True)\n",
    "print(f\"Trained for {med_unet.time_horizon} min time horizon\")\n",
    "med_unet.create_dataloaders(\n",
    "    dataset=\"goes16\",\n",
    "    path=\"../datasets/goes16/salto/\",\n",
    "    batch_size=1,\n",
    "    time_horizon=med_unet.time_horizon,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161ad9ec-0a71-4cd2-b4b4-4516298c1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_crps_list = []\n",
    "crps_loss = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_batch_idx, (in_frames, out_frames) in enumerate(\n",
    "        med_unet.val_loader\n",
    "    ):\n",
    "        in_frames = in_frames.to(device=device)\n",
    "        out_frames = out_frames.to(device=device)\n",
    "\n",
    "        frames_pred = med_unet.model(in_frames.float())\n",
    "        # numeric_crps = med_unet.get_numerical_CRPS(\n",
    "        #     y=out_frames,\n",
    "        #     pred=frames_pred,\n",
    "        #     lower=0,\n",
    "        #     upper=1,\n",
    "        #     count=100,\n",
    "        # )\n",
    "        # numeric_crps_list.append(numeric_crps)\n",
    "        # print(numeric_crps)\n",
    "\n",
    "        close_crps = crps_laplace(out_frames, frames_pred)\n",
    "        crps_loss.append(close_crps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a677e621-7c9f-44d3-ae89-aae3786ccd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2271, device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f071adaf-c1e2-4e25-8897-cc032770093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6022)\n",
      "tensor(0.1377)\n",
      "tensor(642.4297)\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(torch.tensor(crps_loss)))\n",
    "print(torch.mean(torch.tensor(crps_loss)))\n",
    "print(torch.sum(torch.tensor(crps_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb36050-3fd9-492d-a34a-7b62da99c9fb",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "146f6458-5039-4d51-b1a7-9c59869c8068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for 60 min time horizon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GOES16Dataset:Number of sequences filtered: 614\n",
      "INFO:GOES16Dataset:Number of sequences filtered by black images: 1\n",
      "INFO:GOES16Dataset:Number of sequences filtered: 192\n",
      "INFO:GOES16Dataset:Number of sequences filtered by black images: 1\n",
      "INFO:MeanStdUNet:Train loader size: 23247\n",
      "INFO:MeanStdUNet:Val loader size: 4666\n",
      "INFO:MeanStdUNet:Samples height: 1024, Samples width: 1024\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"../checkpoints/goes16/mean_TH60_SC256/mean/MeanStdUNet_IN3_F32_SC256_BS_4_TH60_E9_BVMtens_D2024-11-07_04:21.pt\"\n",
    "\n",
    "unet_config = UNetConfig(\n",
    "    in_frames=3,\n",
    "    spatial_context=256,\n",
    "    filters=32,\n",
    "    output_activation=\"sigmoid\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "mean_unet = MeanStdUNet(\n",
    "    config=unet_config,\n",
    ")\n",
    "\n",
    "mean_unet.load_checkpoint(checkpoint_path, device, eval_mode=True)\n",
    "print(f\"Trained for {mean_unet.time_horizon} min time horizon\")\n",
    "mean_unet.create_dataloaders(\n",
    "    dataset=\"goes16\",\n",
    "    path=\"../datasets/goes16/salto/\",\n",
    "    batch_size=1,\n",
    "    time_horizon=mean_unet.time_horizon,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9ac6c77-8c63-445b-bd9f-5aebad998b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-inf -inf -inf ... -inf  inf -inf]\n",
      "  [-inf -inf -inf ...  inf  inf -inf]\n",
      "  [-inf -inf -inf ...  inf  inf  inf]\n",
      "  ...\n",
      "  [ inf  inf -inf ... -inf -inf -inf]\n",
      "  [ inf -inf -inf ... -inf -inf -inf]\n",
      "  [-inf -inf -inf ... -inf -inf -inf]]]\n",
      "tensor([[[0.1009, 0.1172, 0.1060,  ..., 0.2520, 0.2878, 0.1333],\n",
      "         [0.1143, 0.1114, 0.1093,  ..., 0.3679, 0.3396, 0.2487],\n",
      "         [0.1125, 0.1020, 0.0947,  ..., 0.6021, 0.5005, 0.2966],\n",
      "         ...,\n",
      "         [0.6577, 0.5054, 0.2053,  ..., 0.1608, 0.1619, 0.1416],\n",
      "         [0.4751, 0.2181, 0.1646,  ..., 0.1660, 0.1608, 0.1277],\n",
      "         [0.1920, 0.1741, 0.1671,  ..., 0.1619, 0.1494, 0.1167]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[[0.2322, 0.2339, 0.2328,  ..., 0.2769, 0.2776, 0.2769],\n",
      "         [0.2332, 0.2339, 0.2318,  ..., 0.2773, 0.2776, 0.2773],\n",
      "         [0.2319, 0.2313, 0.2292,  ..., 0.2786, 0.2791, 0.2786],\n",
      "         ...,\n",
      "         [0.2668, 0.2693, 0.2727,  ..., 0.2428, 0.2458, 0.2448],\n",
      "         [0.2671, 0.2703, 0.2727,  ..., 0.2460, 0.2469, 0.2448],\n",
      "         [0.2654, 0.2681, 0.2705,  ..., 0.2477, 0.2429, 0.2355]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16) tensor(7.1526e-07, device='cuda:0', dtype=torch.float16) tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "mean_std_loss_per_batch = []\n",
    "crps_gaussian_list = []\n",
    "numeric_crps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_batch_idx, (in_frames, out_frames) in enumerate(\n",
    "        mean_unet.val_loader\n",
    "    ):\n",
    "        in_frames = in_frames.to(device=device, dtype=mean_unet.torch_dtype)\n",
    "        out_frames = out_frames.to(device=device, dtype=mean_unet.torch_dtype)\n",
    "\n",
    "        with torch.autocast(\n",
    "            device_type=\"cuda\", dtype=mean_unet.torch_dtype\n",
    "        ):\n",
    "            frames_pred = mean_unet.model(in_frames)\n",
    "\n",
    "            frames_pred = mean_unet.remove_spatial_context(frames_pred)\n",
    "            mean_std_loss_per_batch.append(\n",
    "                mean_unet.calculate_loss(frames_pred, out_frames)\n",
    "            )\n",
    "\n",
    "            crps_gaussian_list.append(\n",
    "                crps_gaussian(\n",
    "                    out_frames[:, 0, :, :],\n",
    "                    frames_pred[:, 0, :, :],\n",
    "                    frames_pred[:, 1, :, :],\n",
    "                )\n",
    "            )\n",
    "            print(out_frames[:, 0, :, :])\n",
    "            print(frames_pred[:, 0, :, :])\n",
    "            print(frames_pred[:, 1, :, :])\n",
    "            \n",
    "            print(torch.min(frames_pred[:, 1, :, :]), torch.max(frames_pred[:, 1, :, :]), torch.mean(frames_pred[:, 1, :, :]))\n",
    "            print(crps_gaussian_list[-1])\n",
    "        break\n",
    "            # numeric_crps.append(\n",
    "            #     mean_unet.get_numerical_CRPS(\n",
    "            #         y=out_frames, pred=frames_pred, lower=0., upper=1., count=100\n",
    "            #     )\n",
    "            # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab965655-6ced-4eeb-9fc7-9116ee131620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33a9a314-1e67-4c1e-84a2-031aafa49b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7673936486244202\n",
      "CRPS: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "CRPS: nan\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation loss: {torch.mean(torch.tensor(mean_std_loss_per_batch))}\")\n",
    "print(f\"CRPS: {(crps_gaussian_list)}\")\n",
    "print(f\"CRPS: {np.mean(crps_gaussian_list)}\")\n",
    "# print(f\"NUMERIC CRPS: {np.mean(numeric_crps)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2accf79c-dded-4e5a-9e05-9572db065dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
